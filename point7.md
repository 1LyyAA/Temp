# Пункт 7: Градиентный спуск

## Основные идеи:

1. **Принцип работы:**
   - Итеративный метод оптимизации
   - Движение в направлении антиградиента
   - ω⁽ᵗ⁺¹⁾ = ω⁽ᵗ⁾ - α∇J(ω⁽ᵗ⁾)

2. **Скорость обучения α:**
   - Контролирует размер шага
   - Слишком большая → не сходится
   - Слишком маленькая → медленная сходимость

3. **Градиенты для разных функций потерь:**
   - MSE: ∇J = -2/n Xᵀ(y - Xω)
   - Ridge: ∇J = -2/n Xᵀ(y - Xω) + 2λω
   - Lasso: ∇J = -2/n Xᵀ(y - Xω) + λ·sign(ω)
   - MAE: использует субградиент

4. **Преимущества:**
   - Работает для больших датасетов
   - Не требует вычисления (XᵀX)⁻¹
   - Подходит для онлайн-обучения
