# Пункт 8: Стохастический градиентный спуск (SGD)

## Основные идеи:

1. **Отличие от обычного градиентного спуска:**
   - Использует один случайный объект (или мини-батч)
   - Обновление после каждого объекта
   - ω⁽ᵗ⁺¹⁾ = ω⁽ᵗ⁾ - α∇Jᵢ(ω⁽ᵗ⁾)

2. **Преимущества SGD:**
   - Быстрее для больших датасетов
   - Меньше требований к памяти
   - Может избежать локальных минимумов
   - Подходит для онлайн-обучения

3. **Градиент для одного объекта:**
   - MSE: ∇Jᵢ = -2xᵢ(yᵢ - xᵢᵀω)
   - Вычисляется значительно быстрее

4. **Особенности:**
   - Более шумная траектория оптимизации
   - Требует тщательной настройки скорости обучения
   - Часто используют мини-батчи (компромисс)
